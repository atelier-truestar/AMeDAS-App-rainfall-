"""ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«"""
import os
import re
import time
from typing import Dict, List, Optional, Tuple

import numpy as np
import pandas as pd
import snowflake.permissions as permissions  # type: ignore
from snowflake.snowpark import Session
from snowflake.snowpark.context import get_active_session
from snowflake.snowpark.exceptions import SnowparkSQLException

import streamlit as st


class DataFetcher:
   """DataFetcherã‚¯ãƒ©ã‚¹

   ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã™ã‚‹ãŸã‚ã®ã‚¯ãƒ©ã‚¹
   """

   def __init__(self, session: Session) -> None:
      """ã‚³ãƒ³ã‚¹ãƒˆãƒ©ã‚¯ã‚¿

      :arg session: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚»ãƒƒã‚·ãƒ§ãƒ³
      """
      try:
         self.session: Session = session
      except Exception as e:
         st.error("ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
         st.error(f"ã‚¨ãƒ©ãƒ¼å†…å®¹: {str(e)}")

   def get_daily_nearest_observatory(self) -> pd.DataFrame:
      """æ—¥æ¬¡ã®æœ€å¯„ã‚Šè¦³æ¸¬æ‰€ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã™ã‚‹

      :return: æœ€å¯„ã‚Šã®è¦³æ¸¬æ‰€ãƒ‡ãƒ¼ã‚¿ã®DataFrame
      """
      query: str = "SELECT * FROM PUBLIC.DAILY_OBSERVATORY_RAINFALL"

      try:
         daily_amedas_df = self.session.sql(query).to_pandas()
         return daily_amedas_df

      except Exception as e:
         st.error("æœ€å¯„ã‚Šè¦³æ¸¬æ‰€ãƒ‡ãƒ¼ã‚¿ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
         st.error(f"ã‚¨ãƒ©ãƒ¼å†…å®¹: {str(e)}")
         return pd.DataFrame()  # å¤±æ•—æ™‚ã¯ç©ºã®DataFrameã‚’è¿”ã™

   def get_daily_amedas(self) -> pd.DataFrame:
      """æ—¥æ¬¡ã®AMeDASãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã™ã‚‹

      :return: AMeDASãƒ‡ãƒ¼ã‚¿ã®DataFrame
      """
      try:
         # ã‚¯ã‚¨ãƒªã®æ§‹ç¯‰
         query = "SELECT OBSERVATORY_NAME, DATE, RAINFALL_DAILY_TOTAL FROM PUBLIC.DAILY_AMEDAS"
         # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼ã®ä½œæˆ
         cache_key = 'amedasdata'
         if cache_key not in st.session_state:
               st.session_state[cache_key] = self.session.sql(query).to_pandas()
         return st.session_state[cache_key]

      except Exception as e:
         st.error("AMeDASãƒ‡ãƒ¼ã‚¿ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
         st.error(f"ã‚¨ãƒ©ãƒ¼å†…å®¹: {str(e)}")
         return pd.DataFrame()  # å¤±æ•—æ™‚ã¯ç©ºã®DataFrameã‚’è¿”ã™

   @staticmethod
   def get_min_max_date() -> Tuple[str, str]:
      """æ—¥æ¬¡ã®ãƒ‡ãƒ¼ã‚¿ã®æœ€å°æ—¥ä»˜ã¨æœ€å¤§æ—¥ä»˜ã‚’å–å¾—ã™ã‚‹

      :arg session: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚»ãƒƒã‚·ãƒ§ãƒ³
      :return: æœ€å°æ—¥ä»˜ã¨æœ€å¤§æ—¥ä»˜ã®ã‚¿ãƒ—ãƒ«
      """
      query: str = "SELECT MIN(DATE), MAX(DATE) FROM PUBLIC.DAILY_AMEDAS"
      try:
         min_date, max_date = session.sql(query).to_pandas().iloc[0]
         return min_date, max_date
      except Exception as e:
         st.error("æ—¥ä»˜ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
         st.error(f"ã‚¨ãƒ©ãƒ¼å†…å®¹: {str(e)}")
         return "", ""  # å¤±æ•—æ™‚ã¯ç©ºæ–‡å­—åˆ—ã‚’è¿”ã™

class DataProcessor:
   """DataProcessorã‚¯ãƒ©ã‚¹

   ãƒ‡ãƒ¼ã‚¿ã®å‡¦ç†ã¨åŠ å·¥ã‚’è¡Œã†ã‚¯ãƒ©ã‚¹
   """
   replacement_patterns: Dict[str, str] = {
      r"å­—|å¤§å­—|å°å­—": "",
      r"é¬®é‡å·|ãã˜é‡å·|ãã˜ã®å·": "ãã˜é‡å·",
      r"é€šã‚Š|ã¨ãŠã‚Š": "é€šã‚Š",
      r"æŸ¿ç¢•ç”º|æŸ¿ã•ãç”º": "æŸ¿ç¢•ç”º",
      r"åŸ é ­|ãµé ­": "åŸ é ­",
      r"ç•ªç”º|ç•ªä¸": "ç•ªç”º",
      r"å¤§å†|å¤§å®œ": "å¤§å®œ",
      r"ç©|ã•ã„": "ç©",
      r"æ|ãˆã¶ã‚Š": "æ",
      r"è–­|ç¨—|ã²ãˆ|ãƒ’ã‚¨": "ç¨—",
      r"ä¸Šãƒ«|ä¸Šã‚‹": "ä¸Šã‚‹",
      r"ä¸‹ãƒ«|ä¸‹ã‚‹": "ä¸‹ã‚‹",
      r"å››ãƒ„è°·|å››è°·": "å››è°·",
      r"[ä¹‹ãƒã®]": "",
      r"[ï½¹ãƒ¶ã‚±ãŒ]": "ãŒ",
      r"[ï½¶ãƒµã‚«ã‹åŠ›]": "ã‹",
      r"[ï¾‚ãƒƒãƒ„ã£ã¤]": "ã¤",
      r"[ãƒ‹äºŒ]": "äºŒ",
      r"[ãƒå…«]": "å…«"
   }

   def __init__(self, input_df: pd.DataFrame, observatory_df: pd.DataFrame):
      """ã‚³ãƒ³ã‚¹ãƒˆãƒ©ã‚¯ã‚¿

      :arg input_df: å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã®DataFrame
      :arg observatory_df: è¦³æ¸¬æ‰€ãƒ‡ãƒ¼ã‚¿ã®DataFrame
      """
      self.input_df: pd.DataFrame = input_df
      self.observatory_df: pd.DataFrame = observatory_df

   def process_input_data(self, address_column: str) -> pd.DataFrame:
      """å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã‚’å‡¦ç†ã—ã€ä½æ‰€ãƒ‡ãƒ¼ã‚¿ã‚’æ­£è¦åŒ–ã™ã‚‹

      :arg address_column: ä½æ‰€ãƒ‡ãƒ¼ã‚¿ãŒå«ã¾ã‚Œã‚‹åˆ—å
      :return: å‡¦ç†ã•ã‚ŒãŸä½æ‰€ãƒ‡ãƒ¼ã‚¿ã®DataFrame
      """
      try:
         # ç‰¹å®šã®åˆ—ãŒå­˜åœ¨ã—ãªã„å ´åˆã®ã‚¨ãƒ©ãƒ¼ã‚’ã‚­ãƒ£ãƒƒãƒ
         self.input_df['CLEANSED_ADDRESS'] = self.input_df[address_column]\
            .str.replace(r'[!?/:@[\]`{\}~ ã€€]','',regex=True)

      except KeyError as e:
         st.error(f"æŒ‡å®šã•ã‚ŒãŸåˆ—å '{address_column}' ãŒå­˜åœ¨ã—ã¾ã›ã‚“ã€‚")
         st.error(f"ã‚¨ãƒ©ãƒ¼å†…å®¹: {str(e)}")
         return

      try:
         # ä½æ‰€ãƒ‡ãƒ¼ã‚¿ã®å‡¦ç†
         self.input_df['CLEANSED_ADDRESS'] = self.input_df['CLEANSED_ADDRESS'].apply(self.replace_old_kanji)
         self.input_df['CLEANSED_ADDRESS'] = self.input_df['CLEANSED_ADDRESS'].apply\
            (lambda x: self.replace_patterns(x, self.replacement_patterns))
      except AttributeError as e:
         st.error("ä½æ‰€ãƒ‡ãƒ¼ã‚¿ã®å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚")
         st.error(f"ã‚¨ãƒ©ãƒ¼å†…å®¹: {str(e)}")
         return

      pattern: str = (
         r'^(?P<Pref_name>(?:æ±äº¬éƒ½|äº¬éƒ½åºœ|å¤§é˜ªåºœ|.+?[éƒ½é“åºœçœŒ]))?'  # éƒ½é“åºœçœŒåãŒçœç•¥ã•ã‚Œã¦ã„ã¦ã‚‚OK
         r'(?P<City_name>(?:(?:äº¬éƒ½|æœ­å¹Œ|ç¦å²¡|ç”°æ‘|æ±æ‘å±±|æ­¦è”µæ‘å±±|ç¾½æ‘|åæ—¥ç”º|é‡ã€…å¸‚|å¤§ç”º|è’²éƒ¡|å››æ—¥å¸‚|å¤§å’Œéƒ¡å±±|å»¿æ—¥å¸‚|å¤§æ‘)å¸‚|.\
            +?éƒ¡(?:ç‰æ‘|å¤§ç”º|.+?)[ç”ºæ‘]|.+?å¸‚.+?åŒº|.+?[å¸‚åŒºç”ºæ‘]))?'  # å¸‚åŒºéƒ¡åãŒçœç•¥ã•ã‚Œã¦ã„ã¦ã‚‚OK
         r'(?P<Street_name>.*)'  # ç•ªåœ°ã‚„ç”ºå
      )

      try:
         # æ­£è¦è¡¨ç¾ã§ã®ä½æ‰€åˆ†å‰²
         splitted_df: pd.DataFrame = self.input_df['CLEANSED_ADDRESS'].str.extract(pattern)
      except Exception as e:
         st.error("ä½æ‰€ã®æ­£è¦è¡¨ç¾è§£æä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚")
         st.error(f"ã‚¨ãƒ©ãƒ¼å†…å®¹: {str(e)}")
         return

      # å¸‚åŒºç”ºæ‘åã¨éƒ½é“åºœçœŒåãŒåŒä¸€ã®å ´åˆã®å‡¦ç†
      try:
         splitted_df.loc[(splitted_df['City_name'] == splitted_df['Pref_name']), 'City_name'] = 'XXX'
         splitted_df.columns = ['Pref name', 'City name', 'Street name']
         splitted_df['Street name'] = splitted_df['Street name'].apply(self.normalize_address)
         splitted_df['Street name'] = splitted_df['Street name'].astype(str).str.replace(r'\d+.*', '', regex=True)
         splitted_df = splitted_df.fillna('XXX')
         splitted_df.replace('', 'XXX', inplace=True)
      except Exception as e:
         st.error("ä½æ‰€ãƒ‡ãƒ¼ã‚¿ã®åŠ å·¥ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚")
         st.error(f"ã‚¨ãƒ©ãƒ¼å†…å®¹: {str(e)}")
         return

      return self.join_dfs(splitted_df)

   @staticmethod
   def replace_old_kanji(address: str) -> str:
      """æ—§å­—ä½“ã‚’æ–°å­—ä½“ã«ç½®æ›ã™ã‚‹

      :arg address: ç½®æ›å‰ã®ä½æ‰€æ–‡å­—åˆ—
      :return: ç½®æ›å¾Œã®ä½æ‰€æ–‡å­—åˆ—
      """
      # æ—§å­—ä½“ã¨æ–°å­—ä½“ã®ãƒªã‚¹ãƒˆ
      jis_old_kanji: List[str] = (
         "äº,åœ,å£¹,æ¦®,é©›,æ‡‰,æ«»,å‡,æœƒ,æ‡·,è¦º,æ¨‚,é™·,æ­¡,æ°£,æˆ²,æ“š,æŒ¾,å€,å¾‘,æºª,è¼•,è—,å„‰,åœˆ,æ¬Š,åš´,æ†,åœ‹,é½‹,é›œ,è ¶,æ®˜,å…’,å¯¦,é‡‹,å¾,ç¸±,æ•,ç‡’,æ¢,å‰©,å£¤,é‡€,çœ,ç›¡,é†‰,é«“,è²,ç«Š,"
         "æ·º,éŒ¢,ç¦ª,çˆ­,æ’,é¨·,å±¬,å°,æ»¯,æ“‡,å–®,æ–·,ç™¡,é‘„,æ••,éµ,å‚³,é»¨,é¬ª,å±†,è…¦,å»¢,ç™¼,è »,æ‹‚,é‚Š,ç“£,å¯¶,æ²’,æ»¿,è—¥,é¤˜,æ¨£,äº‚,å…©,ç¦®,éˆ,çˆ,ç£,æƒ¡,é†«,é£®,ç‡Ÿ,åœ“,æ­,å¥§,åƒ¹,ç¹ª,æ“´,å­¸,"
         "ç½,å‹¸,è§€,æ­¸,çŠ§,æ“§,ç‹¹,é©…,è–,ç¶“,ç¹¼,ç¼º,åŠ,æª¢,é¡¯,å»£,é‘›,ç¢,åŠ‘,åƒ,æ…˜,çµ²,è¾­,èˆ,å£½,æ¾,è‚…,å°‡,è­‰,ä¹˜,ç–Š,å­ƒ,è§¸,å¯¢,åœ–,ç©—,æ¨,é½Š,æ”,æˆ°,æ½›,é›™,èŠ,è£,è—,çºŒ,é«”,è‡º,æ¾¤,è†½,"
         "å½ˆ,èŸ²,å»³,é­,é»,ç‡ˆ,ç›œ,ç¨,è²³,éœ¸,è³£,é«®,ç¥•,ä½›,è®Š,è¾¯,è±,é£œ,é»˜,èˆ‡,è­½,è¬ ,è¦½,çµ,å‹µ,é½¡,å‹,å£“,çˆ²,éš±,è¡,é¹½,æ¯†,ç©©,ç•«,å£,æ®¼,å¶½,å·,é—œ,é¡,åƒ,èˆŠ,å³½,æ›‰,å‹³,æƒ ,è¢,é·„,ç¸£,"
         "éšª,ç»,é©—,æ•ˆ,è™Ÿ,æ¿Ÿ,å†Œ,æ£§,è´Š,é½’,æ¿•,å¯«,æ”¶,ç¸,è™•,ç¨±,å¥¬,æ·¨,ç¹©,è®“,å›‘,æ„¼,ç²¹,éš¨,æ•¸,éœ,å°ˆ,è¸,çº–,å£¯,æœ,ç¸½,è‡Ÿ,å¢®,å¸¶,ç€§,æ“”,åœ˜,é²,æ™,è½,é,è½‰,ç•¶,ç¨»,è®€,æƒ±,æ‹œ,éº¥,æ‹”,"
         "æ¿±,ç«,è¾¨,èˆ–,è¥ƒ,è¬,è­¯,è±«,æ–,ä¾†,é¾,å£˜,éš¸,æˆ€,æ¨“,é°º,é¶¯,è £,æ”ª,ç«ˆ,çŒ,è««,é ¸,ç¤¦,è˜‚,é±,è³¤,å£º,ç¤ª,æª®,æ¿¤,é‚‡,è …,æªœ,å„˜,è—ª,ç± ,å½Œ,éº©,æ ,å¡š,æ·µ,èˆŸ,æœƒ"
      ).split(",")

      jis_new_kanji: List[str] = (
         "äºœ,å›²,å£±,æ „,é§…,å¿œ,æ¡œ,ä»®,ä¼š,æ‡,è¦š,æ¥½,é™¥,æ­“,æ°—,æˆ¯,æ‹ ,æŒŸ,åŒº,å¾„,æ¸“,è»½,èŠ¸,å€¹,åœ,æ¨©,å³,æ’,å›½,æ–,é›‘,èš•,æ®‹,å…,å®Ÿ,é‡ˆ,å¾“,ç¸¦,å™,ç„¼,æ¡,å‰°,å£Œ,é†¸,çœŸ,å°½,é…”,é«„,å£°,çªƒ,"
         "æµ…,éŠ­,ç¦…,äº‰,æŒ¿,é¨’,å±,å¯¾,æ»,æŠ,å˜,æ–­,ç—´,é‹³,å‹…,é‰„,ä¼,å…š,é—˜,å±Š,è„³,å»ƒ,ç™º,è›®,æ‰•,è¾º,å¼,å®,æ²¡,æº€,è–¬,ä½™,æ§˜,ä¹±,ä¸¡,ç¤¼,éœŠ,ç‚‰,æ¹¾,æ‚ª,åŒ»,é£²,å–¶,å††,æ¬§,å¥¥,ä¾¡,çµµ,æ‹¡,å­¦,"
         "ç¼¶,å‹§,è¦³,å¸°,çŠ ,æŒ™,ç‹­,é§†,èŒ,çµŒ,ç¶™,æ¬ ,å‰£,æ¤œ,é¡•,åºƒ,é‰±,ç •,å‰¤,å‚,æƒ¨,ç³¸,è¾,èˆ,å¯¿,æ¸‹,ç²›,å°†,è¨¼,ä¹—,ç•³,å¬¢,è§¦,å¯,å›³,ç©‚,æ¢,æ–‰,æ‘‚,æˆ¦,æ½œ,åŒ,è˜,è£…,è”µ,ç¶š,ä½“,å°,æ²¢,èƒ†,"
         "å¼¾,è™«,åº,é®,ç‚¹,ç¯,ç›—,ç‹¬,å¼,è¦‡,å£²,é«ª,ç§˜,ä»,å¤‰,å¼,è±Š,ç¿»,é»™,ä¸,èª‰,è¬¡,è¦§,çŒŸ,åŠ±,é½¢,åŠ´,åœ§,ç‚º,éš ,è¡›,å¡©,æ®´,ç©,ç”»,å£Š,æ®»,å²³,å·»,é–¢,é¡”,å½,æ—§,å³¡,æš,å‹²,æµ,è›,é¶,çœŒ,"
         "é™º,çŒ®,é¨“,åŠ¹,å·,æ¸ˆ,å†Š,æ¡Ÿ,è³›,æ­¯,æ¹¿,å†™,å,ç£,å‡¦,ç§°,å¥¨,æµ„,ç¸„,è­²,å˜±,æ…,ç²‹,éš,æ•°,é™,å°‚,è·µ,ç¹Š,å£®,æœ,ç·,è‡“,å •,å¸¯,æ»,æ‹…,å›£,é…,æ˜¼,è´,é€“,è»¢,å½“,ç¨²,èª­,æ‚©,æ‹,éº¦,æŠœ,"
         "æµœ,ä¸¦,å¼,èˆ—,è¤’,ä¸‡,è¨³,äºˆ,æº,æ¥,ç«œ,å¡,éš·,æ‹,æ¥¼,é¯µ,é´¬,è›,æ’¹,ç«ƒ,æ½…,è«Œ,é š,ç ¿,è•Š,é­,è³,å£·,ç º,æ¢¼,æ¶›,è¿©,è¿,æ¡§,ä¾­,è–®,ç¯­,å¼¥,éº¸,æŸ³,å¡š,æ¸•,èˆ¹,æ›½"
      ).split(",")
      # æ—§å­—ä½“ã‹ã‚‰æ–°å­—ä½“ã¸ã®ãƒãƒƒãƒ”ãƒ³ã‚°è¾æ›¸ã‚’ä½œæˆ
      kanji_mapping: Dict[str, str] = dict(zip(jis_old_kanji, jis_new_kanji))

      try:
         for old, new in kanji_mapping.items():
               address = address.replace(old, new)
      except Exception as e:
         st.error("æ—§å­—ä½“ã®å¤‰æ›ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚")
         st.error(f"ã‚¨ãƒ©ãƒ¼å†…å®¹: {str(e)}")
      return address

   @staticmethod
   def replace_patterns(address: str, patterns: Dict[str, str]) -> str:
      """ä½æ‰€æ–‡å­—åˆ—ã«å¯¾ã—ã¦ä¸€é€£ã®ç½®æ›ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’é©ç”¨ã™ã‚‹

      :arg address: ç½®æ›å‰ã®ä½æ‰€æ–‡å­—åˆ—
      :arg patterns: é©ç”¨ã™ã‚‹ç½®æ›ãƒ‘ã‚¿ãƒ¼ãƒ³ã®è¾æ›¸
      :return: ç½®æ›å¾Œã®ä½æ‰€æ–‡å­—åˆ—
      """
      try:
         for pattern, replacement in patterns.items():
               address = re.sub(pattern, replacement, address)
         return address
      except Exception as e:
         # ç½®æ›ãƒ‘ã‚¿ãƒ¼ãƒ³ã®é©ç”¨ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆ
         st.error("ç½®æ›ãƒ‘ã‚¿ãƒ¼ãƒ³ã®é©ç”¨ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚")
         st.error(f"ã‚¨ãƒ©ãƒ¼å†…å®¹: {str(e)}")
         return address

   @staticmethod
   def normalize_address(address: str) -> str:
      """ä½æ‰€æ–‡å­—åˆ—ã‚’æ­£è¦åŒ–ã™ã‚‹ï¼ˆæ¼¢æ•°å­—ã‚’ã‚¢ãƒ©ãƒ“ã‚¢æ•°å­—ã«å¤‰æ›ãªã©ï¼‰

      :arg address: æ­£è¦åŒ–å‰ã®ä½æ‰€æ–‡å­—åˆ—
      :return: æ­£è¦åŒ–å¾Œã®ä½æ‰€æ–‡å­—åˆ—
      """
      try:
         # æ­£è¦è¡¨ç¾ã«ã‚ˆã‚‹ä½æ‰€ã®åˆ†å‰²
         match = re.match(r'(.+?)((?:[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åã€‡0-9]+[^0-9]*)+)$', address)
         if not match:
               return address

         town_name, number_part = match.groups()
         numbers: List[str] = re.findall(r'([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åã€‡]+|[0-9]+)', number_part)

         # æ¼¢æ•°å­—ã‚’ã‚¢ãƒ©ãƒ“ã‚¢æ•°å­—ã«å¤‰æ›ã™ã‚‹
         normalized_numbers = [DataProcessor.convert_kanji_to_number(num) for num in numbers]

         normalized: str = f"{town_name}{'-'.join(normalized_numbers)}"
         return normalized
      except Exception as e:
         # æ­£è¦åŒ–ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆ
         st.error("ä½æ‰€ã®æ­£è¦åŒ–ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚")
         st.error(f"ã‚¨ãƒ©ãƒ¼å†…å®¹: {str(e)}")
         return address

   @staticmethod
   def convert_kanji_to_number(num: str) -> str:
      """æ¼¢æ•°å­—ã‚’ã‚¢ãƒ©ãƒ“ã‚¢æ•°å­—ã«å¤‰æ›ã™ã‚‹"""
      kanji_to_number: Dict[str, str] = {
      'ä¸€': '1', 'äºŒ': '2', 'ä¸‰': '3', 'å››': '4', 'äº”': '5',
      'å…­': '6', 'ä¸ƒ': '7', 'å…«': '8', 'ä¹': '9', 'å': '10',
      'ã€‡': '0'
      }
      try:
         if num in kanji_to_number:
               return kanji_to_number[num]
         elif all(char in kanji_to_number for char in num):
               normalized_num = ''
               for char in num:
                  if char == 'å':
                     if normalized_num == '':
                           normalized_num += '10'
                     elif normalized_num[-1] == '1':
                           normalized_num = normalized_num[:-1] + '10'
                     else:
                           normalized_num = normalized_num[:-1] + str(int(normalized_num[-1]) * 10)
                  else:
                     normalized_num += kanji_to_number[char]
               return normalized_num
         else:
               return num
      except Exception as e:
         st.error("æ¼¢æ•°å­—ã®å¤‰æ›ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚")
         st.error(f"ã‚¨ãƒ©ãƒ¼å†…å®¹: {str(e)}")
         raise e

   def find_matching_row(self, row: pd.Series, observatory_df: pd.DataFrame) -> pd.Series:
      """å…¥åŠ›ã•ã‚ŒãŸä½æ‰€ã«æœ€ã‚‚è¿‘ã„è¦³æ¸¬æ‰€ã‚’è¦‹ã¤ã‘ã‚‹

      :arg row: å‡¦ç†ã™ã‚‹ä½æ‰€ãƒ‡ãƒ¼ã‚¿ã®è¡Œ
      :arg observatory_df: è¦³æ¸¬æ‰€ãƒ‡ãƒ¼ã‚¿ã®DataFrame
      :return: ãƒãƒƒãƒã—ãŸè¦³æ¸¬æ‰€ãƒ‡ãƒ¼ã‚¿ã¨ä¸€è‡´ãƒ¬ãƒ™ãƒ«
      """
      try:
         street_match = self._find_street_match(row, observatory_df)
         city_match = self._find_city_match(row, observatory_df)
         prefecture_match = self._find_prefecture_match(row, observatory_df)

         if not prefecture_match.empty and not city_match.empty and not street_match.empty: # Pref â­•ï¸ City â­•ï¸ Street â­•ï¸
               return self._handle_all_matches(row, street_match, city_match, prefecture_match)
         elif prefecture_match.empty and not city_match.empty and not street_match.empty:  # Pref âŒ City â­•ï¸ Street â­•ï¸
               return self._handle_city_street_match(row, street_match, city_match)
         elif not prefecture_match.empty and city_match.empty and not street_match.empty: # Pref â­•ï¸ City âŒ Street â­•ï¸
               return self._handle_prefecture_street_match(row, street_match, prefecture_match)
         elif not prefecture_match.empty and not city_match.empty and street_match.empty: # Pref â­•ï¸ City â­•ï¸ Street âŒ
               return self._handle_prefecture_city_match(row, city_match, prefecture_match)
         elif not prefecture_match.empty and city_match.empty and street_match.empty: # Pref â­•ï¸ City âŒ Street âŒ
               return self._create_matched_row(prefecture_match.iloc[0], 'éƒ½é“åºœçœŒãƒ¬ãƒ™ãƒ«')
         elif prefecture_match.empty and not city_match.empty and street_match.empty: # Pref âŒ City â­•ï¸ Street âŒ
               return self._handle_only_city_match(city_match)
         elif prefecture_match.empty and city_match.empty and not street_match.empty: # Pref âŒ City âŒ Street â­•ï¸
               return self._handle_only_street_match(street_match)
         else: # Pref âŒ City âŒ Street âŒ
               return self._create_error_row('ä½æ‰€å½¢å¼ãŒä¸é©åˆ‡ã§ã™')

      except Exception:
         return self._create_error_row('ä½æ‰€å½¢å¼ãŒä¸é©åˆ‡ã§ã™')

   def _find_street_match(self, row: pd.Series, observatory_df: pd.DataFrame) -> pd.DataFrame:
      # ç”ºä¸ç›®åãŒå«ã¾ã‚Œã‚‹è¡Œã‚’è¿”ã™
      return observatory_df[observatory_df['ADDRESS_NAME'].str.contains(row['Street name'], na=False)]

   def _find_city_match(self, row: pd.Series, observatory_df: pd.DataFrame) -> pd.DataFrame:
      # å¸‚åŒºç”ºæ‘åãŒå«ã¾ã‚Œã‚‹è¡Œã‚’è¿”ã™
      city_name_parts = self._split_city_name(row['City name'])
      return observatory_df[
         observatory_df['ADDRESS_NAME'].str.contains(city_name_parts[0], na=False) |
         observatory_df['ADDRESS_NAME'].str.contains(city_name_parts[1], na=False)
      ]

   def _find_prefecture_match(self, row: pd.Series, observatory_df: pd.DataFrame) -> pd.DataFrame:
      # éƒ½é“åºœçœŒåãŒå«ã¾ã‚Œã‚‹è¡Œã‚’è¿”ã™
      return observatory_df[observatory_df['ADDRESS_NAME'].str.contains(row['Pref name'], na=False)]

   def _split_city_name(self, city_name: str) -> Tuple[str, str]:
      # å¸‚åŒºç”ºæ‘åã‚’åˆ†å‰²ã™ã‚‹
      if 'éƒ¡' in city_name:
         parts = city_name.split('éƒ¡')
         return parts[0], re.split(r'[å¸‚ç”ºæ‘]', parts[1])[0]
      elif 'åŒº' in city_name:
         parts = city_name.split('åŒº')
         return parts[0], parts[0]
      else:
         return city_name, city_name

   def _handle_all_matches(self, row: pd.Series, street_match: pd.DataFrame, \
      city_match: pd.DataFrame, prefecture_match: pd.DataFrame) -> pd.Series:
      # Pref â­•ï¸ City â­•ï¸ Street â­•ï¸
      pcs_match = street_match[
         street_match['ADDRESS_NAME'].str.contains(row['City name'], na=False) &
         street_match['ADDRESS_NAME'].str.contains(row['Pref name'], na=False)
      ]
      pc_match = city_match[city_match['ADDRESS_NAME'].str.contains(row['Pref name'], na=False)]
      ps_match = street_match[street_match['ADDRESS_NAME'].str.contains(row['Pref name'], na=False)]
      cs_match = street_match['ADDRESS_NAME'].str.contains(row['City name'], na=False)

      if not pcs_match.empty:
         return self._create_matched_row(pcs_match.iloc[0], 'ç”ºä¸ç›®ãƒ¬ãƒ™ãƒ«')
      elif not pc_match.empty:
         return self._create_matched_row(pc_match.iloc[0], 'å¸‚åŒºéƒ¡ãƒ¬ãƒ™ãƒ«')
      elif not ps_match.empty:
         return self._handle_prefecture_street_match(row, ps_match, prefecture_match)
      elif cs_match.any():
         return self._handle_city_street_match(row, street_match, city_match)
      else:
         return self._handle_fallback_match(street_match, city_match, prefecture_match)

   def _handle_city_street_match(self, row: pd.Series, street_match: pd.DataFrame, \
      city_match: pd.DataFrame) -> pd.Series:
      # Pref âŒ City â­•ï¸ Street â­•ï¸
      cs_match = street_match[street_match['ADDRESS_NAME'].str.contains(row['City name'], na=False)]
      if not cs_match.empty:
         return self._create_matched_row(cs_match.iloc[0], 'ç”ºä¸ç›®ãƒ¬ãƒ™ãƒ«')
      else:
         return self._handle_fallback_match(street_match, city_match, pd.DataFrame())

   def _handle_prefecture_street_match(self, row: pd.Series, \
      street_match: pd.DataFrame, prefecture_match: pd.DataFrame) -> pd.Series:
      # Pref â­•ï¸ City âŒ Street â­•ï¸
      ps_match = street_match[street_match['ADDRESS_NAME'].str.contains(row['Pref name'], na=False)]
      if not ps_match.empty:
         if len(ps_match) == 1:
               return self._create_matched_row(ps_match.iloc[0], 'ç”ºä¸ç›®ãƒ¬ãƒ™ãƒ«')
         else:
               return self._create_matched_row(ps_match.iloc[0], \
                  'çœŒã¨ç”ºä¸ç›®ãŒãƒ’ãƒƒãƒˆã—ã¾ã—ãŸãŒã€2ä»¶ä»¥ä¸Šã‚ã‚‹ã®ã§ç‰¹å®šã§ãã¦ã„ã¾ã›ã‚“')
      else:
         return self._handle_fallback_match(street_match, pd.DataFrame(), prefecture_match)

   def _handle_prefecture_city_match(self, row: pd.Series, \
      city_match: pd.DataFrame, prefecture_match: pd.DataFrame) -> pd.Series:
      # Pref â­•ï¸ City â­•ï¸ Street âŒ
      pc_match = city_match[city_match['ADDRESS_NAME'].str.contains(row['Pref name'], na=False)]
      if not pc_match.empty:
         return self._create_matched_row(pc_match.iloc[0], 'å¸‚åŒºéƒ¡ãƒ¬ãƒ™ãƒ«')
      else:
         return self._handle_fallback_match(pd.DataFrame(), city_match, prefecture_match)

   def _handle_only_city_match(self, city_match: pd.DataFrame) -> pd.Series:
      # Pref âŒ City â­•ï¸ Street âŒ
      if len(city_match) == 1:
         return self._create_matched_row(city_match.iloc[0], 'å¸‚åŒºéƒ¡ãƒ¬ãƒ™ãƒ«')
      else:
         return self._create_matched_row(city_match.iloc[0],
                                         'å¸‚åŒºéƒ¡ã®ã¿ãŒãƒ’ãƒƒãƒˆã—ã¾ã—ãŸãŒã€2ä»¶ä»¥ä¸Šã‚ã‚‹ã®ã§ç‰¹å®šã§ãã¦ã„ã¾ã›ã‚“')

   def _handle_only_street_match(self, street_match: pd.DataFrame) -> pd.Series:
      # Pref âŒ City âŒ Street â­•ï¸
      if len(street_match) == 1:
         return self._create_matched_row(street_match.iloc[0], 'ç”ºä¸ç›®ãƒ¬ãƒ™ãƒ«')
      else:
         return self._create_matched_row(street_match.iloc[0], \
            'ç”ºä¸ç›®ã®ã¿ãŒãƒ’ãƒƒãƒˆã—ã¾ã—ãŸãŒã€2ä»¶ä»¥ä¸Šã‚ã‚‹ã®ã§ç‰¹å®šã§ãã¦ã„ã¾ã›ã‚“')

   def _handle_fallback_match(self, street_match: pd.DataFrame, \
      city_match: pd.DataFrame, prefecture_match: pd.DataFrame) -> pd.Series:
      # Pref âŒ City âŒ Street âŒ
      if len(street_match) == 1:
         return self._create_matched_row(street_match.iloc[0], 'ç”ºä¸ç›®ãƒ¬ãƒ™ãƒ«')
      elif len(city_match) == 1:
         return self._create_matched_row(city_match.iloc[0], 'å¸‚åŒºéƒ¡ãƒ¬ãƒ™ãƒ«')
      elif not prefecture_match.empty:
         return self._create_matched_row(prefecture_match.iloc[0], 'éƒ½é“åºœçœŒãƒ¬ãƒ™ãƒ«')
      else:
         return self._create_error_row('ä½æ‰€å½¢å¼ãŒä¸é©åˆ‡ã§ã™')

   def _create_matched_row(self, matched_row: pd.Series, match_level: str) -> pd.Series:
      # ãƒãƒƒãƒã—ãŸè¡Œã«MATCH_LEVELåˆ—ã‚’è¿½åŠ 
      result = matched_row.copy()
      result['MATCH_LEVEL'] = match_level
      return result

   def _create_error_row(self, error_message: str) -> pd.Series:
      # ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å«ã‚€è¡Œã‚’ä½œæˆ
      matched_row = pd.Series([np.nan] * (len(self.observatory_df.columns) + 1),
                              index=self.observatory_df.columns.tolist() + ['MATCH_LEVEL'])
      matched_row['NEAREST_OBSERVATORY'] = 'XXX'
      matched_row['MATCH_LEVEL'] = error_message
      return matched_row

   def join_dfs(self, splitted_df: pd.DataFrame) -> pd.DataFrame:
      """åˆ†å‰²ã•ã‚ŒãŸä½æ‰€ãƒ‡ãƒ¼ã‚¿ã¨è¦³æ¸¬æ‰€ãƒ‡ãƒ¼ã‚¿ã‚’çµåˆã™ã‚‹

      :arg splitted_df: åˆ†å‰²ã•ã‚ŒãŸä½æ‰€ãƒ‡ãƒ¼ã‚¿ã®DataFrame
      :return: çµåˆã•ã‚ŒãŸDataFrame
      """
      try:
         # splitted_dfã®å„è¡Œã«å¯¾ã—ã¦find_matching_rowã‚’é©ç”¨
         matching_rows: pd.DataFrame = splitted_df.apply\
            (lambda row: self.find_matching_row(row, self.observatory_df), axis=1)

         try:
            # çµåˆå‡¦ç†
            result_df = pd.concat([splitted_df.reset_index(drop=True),
                                 matching_rows.reset_index(drop=True)],
                                 axis=1)
            return result_df

         except Exception as e:
            # çµåˆå‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆ
            st.error("ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã®çµåˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚")
            st.error(f"ã‚¨ãƒ©ãƒ¼å†…å®¹: {str(e)}")
            return splitted_df  # çµåˆãŒå¤±æ•—ã—ãŸå ´åˆã¯ã€å…ƒã®åˆ†å‰²ãƒ‡ãƒ¼ã‚¿ã‚’è¿”ã™

      except Exception as e:
         # applyé–¢æ•°ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆ
         st.error("è¦³æ¸¬æ‰€ãƒ‡ãƒ¼ã‚¿ã¨ã®ç…§åˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚")
         st.error(f"ã‚¨ãƒ©ãƒ¼å†…å®¹: {str(e)}")
         return splitted_df  # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆã€å…ƒã®åˆ†å‰²ãƒ‡ãƒ¼ã‚¿ã‚’è¿”ã™

class UIHandler:
   """UIHandlerã‚¯ãƒ©ã‚¹

   Streamlitã‚’ä½¿ç”¨ã—ãŸUIã®å‡¦ç†ã‚’è¡Œã†ã‚¯ãƒ©ã‚¹
   """
   def __init__(self, session: Session) -> None:
      """ã‚³ãƒ³ã‚¹ãƒˆãƒ©ã‚¯ã‚¿

      :arg session: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚»ãƒƒã‚·ãƒ§ãƒ³
      """
      try:
         self.session: Session = session
      except Exception as e:
         st.error("ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
         st.error(f"ã‚¨ãƒ©ãƒ¼å†…å®¹: {str(e)}")

   def setup_page(self) -> None:
      """ãƒšãƒ¼ã‚¸ã®åŸºæœ¬è¨­å®šã‚’è¡Œã†"""
      try:
         st.set_page_config(
               page_title="AMeDAS Data Maker",  # ãƒšãƒ¼ã‚¸ã®ã‚¿ã‚¤ãƒˆãƒ«ã‚’è¨­å®š
               page_icon="ğŸŒ¤",  # ãƒšãƒ¼ã‚¸ã®ã‚¢ã‚¤ã‚³ãƒ³ã‚’è¨­å®š
               layout="wide"  # ãƒšãƒ¼ã‚¸ã®ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆã‚’åºƒã‚ã«è¨­å®š
         )
      except Exception as e:
         st.error("ãƒšãƒ¼ã‚¸è¨­å®šä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚")
         st.error(f"ã‚¨ãƒ©ãƒ¼å†…å®¹: {str(e)}")

      # CSSãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿
      try:
         css_file = os.path.join(os.path.dirname(__file__), 'style.css')
         with open(css_file, 'r', encoding='utf-8') as f:
               css = f.read()
         st.markdown(f'<style>{css}</style>', unsafe_allow_html=True)
      except Exception as e:
         st.error("CSSãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚")
         st.error(f"ã‚¨ãƒ©ãƒ¼å†…å®¹: {str(e)}")

      st.markdown("# â˜” AMeDAS Data Maker (Specialized in RAINFALL) â˜”")
      st.markdown("##### é¸æŠã—ãŸãƒ†ãƒ¼ãƒ–ãƒ«ã®ä½æ‰€ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰é™æ°´é‡ãƒ‡ãƒ¼ã‚¿ã‚’ç´ã¥ã‘ã™ã‚‹ã‚¢ãƒ—ãƒª")

   def select_table(self, amedas_maker: 'AMeDASDataMaker') -> bool:
      """èª­ã¿è¾¼ã‚€ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’é¸æŠã™ã‚‹UIã‚’è¡¨ç¤ºã™ã‚‹"""
      st.subheader("1ï¸âƒ£ èª­ã¿è¾¼ã‚€ãƒ†ãƒ¼ãƒ–ãƒ«ã®é¸æŠ")
      try:
         is_input_table_set = amedas_maker.check_input_table_ref_existence()
      except Exception as e:
         st.error("ãƒ†ãƒ¼ãƒ–ãƒ«å‚ç…§ã®ç¢ºèªä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚")
         st.error(f"ã‚¨ãƒ©ãƒ¼å†…å®¹: {str(e)}")
         return False

      if st.button("ãƒ†ãƒ¼ãƒ–ãƒ«ã®é¸æŠ"):
         try:
            for key in list(st.session_state.keys()):
               del st.session_state[key]
            permissions.request_reference(amedas_maker.input_table_ref)

         except Exception as e:
            st.error("ãƒ†ãƒ¼ãƒ–ãƒ«é¸æŠä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚")
            st.error(f"ã‚¨ãƒ©ãƒ¼å†…å®¹: {str(e)}")

      if is_input_table_set:
         st.success("ãƒ†ãƒ¼ãƒ–ãƒ«ãŒé¸æŠã•ã‚Œã¾ã—ãŸ", icon="âœ…")
         try:
            # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒ–ã•ã‚ŒãŸã‚¯ã‚¨ãƒªã‚’ä½¿ç”¨
            reference_query = "SELECT * FROM reference(?);"
            test_query = "SELECT * FROM reference(?) LIMIT 5;"
            query = test_query if test_mode else reference_query
            input_df = self.session.sql(query, params=(amedas_maker.input_table_ref,)).to_pandas()
         except Exception as e:
            st.error("ãƒ†ãƒ¼ãƒ–ãƒ«ãƒ‡ãƒ¼ã‚¿ã®å–å¾—ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚")
            st.error(f"ã‚¨ãƒ©ãƒ¼å†…å®¹: {str(e)}")
            return False

         amedas_maker.input_df = input_df
         st.write("æ³¨æ„ : æœ€åˆã®100è¡Œã®ã¿è¡¨ç¤ºã•ã‚Œã¾ã™")
         st.write(amedas_maker.input_df[:100])
      else:
         st.info("ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’é¸æŠã—ã¦ãã ã•ã„", icon="â„¹")

      return is_input_table_set

   def select_address_column(self, is_input_table_set: bool, input_df: pd.DataFrame) -> Tuple[bool, List[str]]:
      """ä½æ‰€ãƒ‡ãƒ¼ã‚¿ã‚’å«ã‚€ã‚«ãƒ©ãƒ ã‚’é¸æŠã™ã‚‹UIã‚’è¡¨ç¤ºã™ã‚‹"""
      st.subheader("2ï¸âƒ£ ä½æ‰€ãƒ‡ãƒ¼ã‚¿ã‚’ã‚‚ã¤ã‚«ãƒ©ãƒ ã®é¸æŠ")
      is_2_ok = False

      try:
         address_columns = [col for col in st.session_state.get('address_columns', []) if col in input_df.columns]
      except Exception as e:
         st.error("ã‚«ãƒ©ãƒ ã®å–å¾—ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ", icon="â›”")
         st.error(f"ã‚¨ãƒ©ãƒ¼å†…å®¹: {str(e)}")
         return False, []

      if not is_input_table_set:
         st.info("1ï¸âƒ£ ã‚’å®Œäº†ã•ã›ã¦ãã ã•ã„", icon="â„¹")
      else:
         address_columns = st.multiselect("ä½æ‰€ãƒ‡ãƒ¼ã‚¿ã®ã‚«ãƒ©ãƒ ã‚’é¸æŠã—ã¦ãã ã•ã„", \
            input_df.columns, default=address_columns)

         if not address_columns:
               st.error("ä½æ‰€ãƒ‡ãƒ¼ã‚¿ã®ã‚«ãƒ©ãƒ ãŒé¸æŠã•ã‚Œã¦ã„ã¾ã›ã‚“", icon="â›”")
         elif len(address_columns) > 1:
               st.error("ä½æ‰€ãƒ‡ãƒ¼ã‚¿ã®ã‚«ãƒ©ãƒ ã¯1ã¤ã ã‘é¸æŠã—ã¦ãã ã•ã„", icon="â›”")
         else:
               st.success("ä½æ‰€ãƒ‡ãƒ¼ã‚¿ã®ã‚«ãƒ©ãƒ ãŒé¸æŠã•ã‚Œã¾ã—ãŸ", icon="âœ…")
               is_2_ok = True
               st.session_state['address_columns'] = address_columns

      return is_2_ok, address_columns

   def select_date_range(self, is_2_ok: bool) -> Tuple[bool, Optional[pd.Timestamp], Optional[pd.Timestamp]]:
      """è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿ã®ç¨®é¡ã¨ãƒ‡ãƒ¼ã‚¿å–å¾—ç¯„å›²ã‚’é¸æŠã™ã‚‹UIã‚’è¡¨ç¤ºã™ã‚‹"""
      st.subheader("3ï¸âƒ£ æœ€å¯„ã‚Šã®è¦³æ¸¬ç‚¹ã¨æ°—è±¡ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—")
      is_3_ok = False
      start_date: Optional[pd.Timestamp] = None
      end_date: Optional[pd.Timestamp] = None

      if not is_2_ok:
         st.info("2ï¸âƒ£ ã‚’å®Œäº†ã•ã›ã¦ãã ã•ã„", icon="â„¹")
         return is_3_ok, start_date, end_date
      else:
         try:
               # ãƒ‡ãƒ¼ã‚¿å–å¾—ç¯„å›²ã®è¨­å®š
               min_date_str, max_date_str = DataFetcher.get_min_max_date()
               min_date = pd.Timestamp(min_date_str)
               max_date = pd.Timestamp(max_date_str)
               # ã‚­ãƒ¼ã‚’ä½¿ç”¨ã—ã¦ä¸€æ„ã®æ—¥ä»˜å…¥åŠ›ã‚¦ã‚£ã‚¸ã‚§ãƒƒãƒˆã‚’ä½œæˆ
               start_date = st.date_input("ãƒ‡ãƒ¼ã‚¿å–å¾—é–‹å§‹æ—¥",
                                          value=min_date,
                                          min_value=min_date,
                                          max_value=max_date,
                                          key="start_date_input")
               end_date = st.date_input("ãƒ‡ãƒ¼ã‚¿å–å¾—çµ‚äº†æ—¥",
                                       value=max_date,
                                       min_value=min_date,
                                       max_value=max_date,
                                       key="end_date_input")

               # æ—¥ä»˜ã®å¦¥å½“æ€§ãƒã‚§ãƒƒã‚¯
               if start_date is not None and end_date is not None and \
                  pd.Timestamp(start_date) > pd.Timestamp(end_date):
                  st.error("é–‹å§‹æ—¥ãŒçµ‚äº†æ—¥ã‚ˆã‚Šå¾Œã«ãªã£ã¦ã„ã¾ã™ã€‚æ—¥ä»˜ã‚’ä¿®æ­£ã—ã¦ãã ã•ã„ã€‚", icon="â›”")
                  return is_3_ok, None, None

               is_3_ok = True
               return is_3_ok, pd.Timestamp(start_date), pd.Timestamp(end_date)

         except Exception as e:
               st.error("ãƒ‡ãƒ¼ã‚¿é¸æŠä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚", icon="â›”")
               st.error(f"ã‚¨ãƒ©ãƒ¼å†…å®¹: {str(e)}")
               return is_3_ok, None, None

   def execute_final_process(self, amedas_maker: 'AMeDASDataMaker', address_columns: List[str],
                                start_date: Optional[pd.Timestamp], end_date: Optional[pd.Timestamp]) -> None:
      """å®Ÿè¡Œãƒœã‚¿ãƒ³ãŒæŠ¼ã•ã‚ŒãŸéš›ã®å‡¦ç†"""
      if st.button("å®Ÿè¡Œ"):
         start_time: float = time.time()
         with st.spinner('å®Ÿè¡Œä¸­...ãŠå¾…ã¡ãã ã•ã„ ğŸ•’  \
            æ³¨æ„ : å®Ÿè¡Œä¸­ã¯å®Ÿè¡Œãƒœã‚¿ãƒ³ã«è§¦ã‚Œãšã€å‡¦ç†ã‚’ã‚„ã‚Šç›´ã—ãŸã„å ´åˆã¯ãƒšãƒ¼ã‚¸ã‚’ãƒªãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ ğŸ”ƒ'):
            try:
               # æ—¥æ¬¡ãƒ‡ãƒ¼ã‚¿ã®å–å¾—
               amedas_maker.daily_df = amedas_maker.data_fetcher.get_daily_amedas()
               amedas_maker.daily_df['DATE'] = pd.to_datetime(amedas_maker.daily_df['DATE'])
               amedas_maker.daily_df = amedas_maker.daily_df[
                  (amedas_maker.daily_df['DATE'] >= pd.Timestamp(start_date)) &
                  (amedas_maker.daily_df['DATE'] <= pd.Timestamp(end_date))
               ]
               amedas_maker.daily_df['DATE'] = amedas_maker.daily_df['DATE'].dt.date

               # ãƒ‡ãƒ¼ã‚¿å‡¦ç†
               data_processor: DataProcessor = DataProcessor(amedas_maker.input_df, amedas_maker.observatory_df)
               result_df: pd.DataFrame = data_processor.process_input_data(address_columns[0])
               # å…ƒã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä¿æŒã™ã‚‹ãŸã‚ã®åˆ—ã‚’è¿½åŠ 
               amedas_maker.input_df['ORIGINAL_INDEX'] = amedas_maker.input_df.index

               # DataFrameã‚’çµåˆ
               final_df: pd.DataFrame = pd.concat(
                  [amedas_maker.input_df.reset_index(drop=True),
                  result_df[['NEAREST_OBSERVATORY', 'MATCH_LEVEL']].reset_index(drop=True)],
                  axis=1
               )

               # ãƒãƒ¼ã‚¸æ“ä½œã‚’è¡Œã†
               final_df = final_df.merge(
                  amedas_maker.daily_df,
                  how='left',
                  left_on='NEAREST_OBSERVATORY',
                  right_on='OBSERVATORY_NAME'
               )

               # ä¸è¦ãªåˆ—ã‚’å‰Šé™¤
               final_df = final_df.drop(columns=['CLEANSED_ADDRESS', 'OBSERVATORY_NAME'])

               # DATEåˆ—ã§ã‚½ãƒ¼ãƒˆã—ã¤ã¤å…ƒã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹é †ã‚’ä¿æŒã™ã‚‹
               final_df = final_df.sort_values(['ORIGINAL_INDEX', 'DATE']).reset_index(drop=True)

               # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹åˆ—ã‚’å‰Šé™¤
               final_df = final_df.drop(columns=['ORIGINAL_INDEX'])

               # ãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆã¨ãƒ‡ãƒ¼ã‚¿ä¿å­˜ã€çµæœã®è¡¨ç¤º
               st.success("ãƒ‡ãƒ¼ã‚¿å–å¾—å®Œäº†ï¼", icon="âœ…")
               amedas_maker.create_output_table(final_df)
               amedas_maker.save_to_output_table(final_df)
               st.write(f"##### å…ƒãƒ‡ãƒ¼ã‚¿ + æœ€å¯„ã‚Šã®è¦³æ¸¬ç‚¹ + ä¸€è‡´ãƒ¬ãƒ™ãƒ« + æ°—è±¡ãƒ‡ãƒ¼ã‚¿({len(final_df)}è¡Œ)")
               st.write("æ³¨æ„ : æœ€åˆã®100è¡Œã®ã¿è¡¨ç¤ºã—ã¦ã„ã¾ã™")
               styled_df = amedas_maker.highlight_columns('NEAREST_OBSERVATORY', final_df[:100]).format(precision=1)
               st.dataframe(styled_df)
               st.write("##### ç¯„å›²åˆ¥ãƒãƒƒãƒãƒ³ã‚°ç‡")
               st.write(result_df['MATCH_LEVEL'].value_counts(normalize=True)
                        .mul(100)
                        .round(0)
                        .astype(int)
                        .astype(str) + '%')

               end_time: float = time.time()
               execution_time: int = int(end_time - start_time)
               st.write(f"##### å®Ÿè¡Œæ™‚é–“ : {execution_time} ç§’")

               st.write("##### â€» å‡ºåŠ›ç”¨ãƒ†ãƒ¼ãƒ–ãƒ«ã«ä¿å­˜ã•ã‚ŒãŸå…¨ã¦ã®ãƒ‡ãƒ¼ã‚¿ã¯ã€ä»¥ä¸‹ã®ã‚¯ã‚¨ãƒªã‚’ä½¿ç”¨ã—ã¦å‚ç…§ã§ãã¾ã™ã€‚")
               st.code("SELECT * FROM PODB_AMEDAS_RAINFALL_APP.CORE.WITH_AMEDAS", language="SQL")

            except Exception as e:
               st.error("ãƒ‡ãƒ¼ã‚¿å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚", icon="â›”")
               st.error(f"ã‚¨ãƒ©ãƒ¼å†…å®¹: {str(e)}")

class AMeDASDataMaker:
   """AMeDASDataMakerã‚¯ãƒ©ã‚¹

   ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®ãƒ¡ã‚¤ãƒ³ã‚¯ãƒ©ã‚¹
   """

   def __init__(self, session: Session, input_table_ref: str) -> None:
      """AMeDASDataMakerã®ã‚³ãƒ³ã‚¹ãƒˆãƒ©ã‚¯ã‚¿ã€‚ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚„å…¥åŠ›ãƒ†ãƒ¼ãƒ–ãƒ«å‚ç…§åã‚’åˆæœŸåŒ–ã—ã€å¿…è¦ãªã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ç”Ÿæˆ

      :arg session: Snowflakeã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
      :arg input_table_ref: å‚ç…§ã™ã‚‹å…¥åŠ›ãƒ†ãƒ¼ãƒ–ãƒ«ã®åå‰
      """
      self.session = session  # Snowflakeã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
      self.input_table_ref: str = input_table_ref  # å…¥åŠ›ãƒ†ãƒ¼ãƒ–ãƒ«ã®å‚ç…§å
      self.input_df: pd.DataFrame = pd.DataFrame()  # å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã®åˆæœŸåŒ–
      self.data_fetcher: DataFetcher = DataFetcher(session)  # ãƒ‡ãƒ¼ã‚¿å–å¾—ç”¨ã®DataFetcherã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
      self.ui_handler: UIHandler = UIHandler(session)  # UIæ“ä½œç”¨ã®UIHandlerã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
      self.daily_df: pd.DataFrame = pd.DataFrame()  # daily_dfã®åˆæœŸåŒ–
      try:
         self.observatory_df: pd.DataFrame = self.data_fetcher.get_daily_nearest_observatory()

      except SnowparkSQLException as e:
         st.error("è¦³æ¸¬æ‰€ãƒ‡ãƒ¼ã‚¿ã®å–å¾—ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚", icon="â›”")
         st.error(f"ã‚¨ãƒ©ãƒ¼å†…å®¹: {str(e)}")
         self.observatory_df = pd.DataFrame()

   def check_input_table_ref_existence(self) -> bool:
      """å…¥åŠ›ãƒ†ãƒ¼ãƒ–ãƒ«ã®å‚ç…§ãŒå­˜åœ¨ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯ã™ã‚‹

      :return: ãƒ†ãƒ¼ãƒ–ãƒ«ãŒå­˜åœ¨ã™ã‚‹å ´åˆTrueã€ãã†ã§ãªã„å ´åˆFalse
      """
      try:
         ref_objects = permissions.get_reference_associations(self.input_table_ref)
         return len(ref_objects) > 0

      except Exception as e:
         st.error("å…¥åŠ›ãƒ†ãƒ¼ãƒ–ãƒ«ã®å‚ç…§ãƒã‚§ãƒƒã‚¯ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚ã‚·ã‚¹ãƒ†ãƒ ç®¡ç†è€…ã«é€£çµ¡ã—ã¦ãã ã•ã„ã€‚", icon="â›”")
         st.error(f"ã‚¨ãƒ©ãƒ¼å†…å®¹: {str(e)}")
         return False

   def create_output_table(self, final_df: pd.DataFrame) -> None:
      """æŒ‡å®šã•ã‚ŒãŸã‚«ãƒ©ãƒ ã«åŸºã¥ã„ã¦ã€æ–°ã—ã„ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ä½œæˆã™ã‚‹ã€‚

      :arg columns: ä½œæˆã™ã‚‹ãƒ†ãƒ¼ãƒ–ãƒ«ã®ã‚«ãƒ©ãƒ åã®ãƒªã‚¹ãƒˆ
      """
      try:
         # ãƒ†ãƒ¼ãƒ–ãƒ«ãŒå­˜åœ¨ã™ã‚‹ã‹ç¢ºèª
         table_exists_query = """
         SELECT COUNT(*)
         FROM INFORMATION_SCHEMA.TABLES
         WHERE TABLE_SCHEMA = 'CORE' AND TABLE_NAME = 'WITH_AMEDAS'
         """
         table_exists = self.session.sql(table_exists_query).collect()[0][0] > 0

         if table_exists:
               self.session.sql("GRANT DELETE ON TABLE CORE.WITH_AMEDAS TO APPLICATION ROLE app_public;").collect()
               # ãƒ†ãƒ¼ãƒ–ãƒ«ã®ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¯ãƒªã‚¢ï¼ˆãƒ†ãƒ¼ãƒ–ãƒ«ãŒå­˜åœ¨ã™ã‚‹ã“ã¨ãŒå‰æï¼‰
               self.session.sql("DROP TABLE IF EXISTS CORE.WITH_AMEDAS").collect()
               st.warning("å‡ºåŠ›ç”¨ã‚¹ã‚­ãƒ¼ãƒã«ãƒ†ãƒ¼ãƒ–ãƒ«ãŒæ—¢ã«å­˜åœ¨ã—ã¦ã„ãŸãŸã‚ã€ã‚¯ãƒªã‚¢ã—ã¾ã—ãŸã€‚", icon="âš ï¸")

         # SQLçš„ãªå‹ã®ãƒãƒƒãƒ”ãƒ³ã‚°
         type_mapping = {
            'int64': 'NUMBER',
            'int32': 'NUMBER',
            'float64': 'FLOAT',
            'float32': 'FLOAT',
            'object': 'VARCHAR',
            'string': 'VARCHAR',
            'bool': 'BOOLEAN',
            'datetime64[ns]': 'TIMESTAMP',
            'timedelta[ns]': 'INTERVAL',
            'category': 'VARCHAR'
         }

         # çµæœã‚’ä¿å­˜ã™ã‚‹ãƒªã‚¹ãƒˆ
         sql_types = []

         # dtypes ã‚’ final_df ã‹ã‚‰å–å¾—
         dtypes = final_df.dtypes

         # å„åˆ—ã®ãƒ‡ãƒ¼ã‚¿å‹ã‚’ç¢ºèªã—ã€SQLå‹ã«ãƒãƒƒãƒ”ãƒ³ã‚°
         for column in final_df.columns:
            dtype = dtypes[column]

            # datetime64[ns] ã®å‡¦ç†
            if 'datetime64' in str(dtype):
               if 'tz' in str(dtype):
                     sql_type = 'TIMESTAMP WITH TIME ZONE'
               else:
                     # æ™‚åˆ»ãŒå…¨ã¦ 00:00:00 ãªã‚‰ DATE ã¨ã—ã¦æ‰±ã†
                     if final_df[column].dt.time.eq(pd.Timestamp('00:00:00').time()).all():
                        sql_type = 'DATE'
                     else:
                        sql_type = 'TIMESTAMP'
            else:
               sql_type = type_mapping.get(str(dtype), 'VARCHAR')

            # çµæœã‚’è¿½åŠ 
            sql_types.append((column, sql_type))

         # ã‚¯ã‚¨ãƒªã®ãƒ™ãƒ¼ã‚¹éƒ¨åˆ†
         columns_sql = "CREATE TABLE IF NOT EXISTS CORE.WITH_AMEDAS ("

         # å„åˆ—ã¨SQLå‹ã‚’è¿½åŠ 
         for column, sql_type in sql_types:
            columns_sql += f"\"{column}\" {sql_type}, "

         # æœ€å¾Œã®ã‚«ãƒ³ãƒã‚’å‰Šé™¤ã—ã€é–‰ã˜æ‹¬å¼§ã‚’è¿½åŠ 
         columns_sql = columns_sql.rstrip(", ") + ");"

         # ãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆã¨æ¨©é™ä»˜ä¸
         self.session.sql(columns_sql).collect()

         for privilege in ["DELETE", "SELECT", "INSERT"]:
            self.session.sql(
                  f"GRANT {privilege} ON TABLE CORE.WITH_AMEDAS TO APPLICATION ROLE app_public;"
            ).collect()
      except SnowparkSQLException as e:
         st.error("ãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚æ¨©é™ãŒä¸è¶³ã—ã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚", icon="â›”")
         st.error(f"ã‚¨ãƒ©ãƒ¼å†…å®¹: {str(e)}")
      except Exception as e:
         st.error("äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚ã‚·ã‚¹ãƒ†ãƒ ç®¡ç†è€…ã«é€£çµ¡ã—ã¦ãã ã•ã„ã€‚", icon="â›”")
         st.error(f"ã‚¨ãƒ©ãƒ¼å†…å®¹: {str(e)}")

   def save_to_output_table(self, df: pd.DataFrame) -> None:
      """æŒ‡å®šã•ã‚ŒãŸDataFrameã‚’Snowflakeã®ãƒ†ãƒ¼ãƒ–ãƒ«ã«ä¿å­˜ã™ã‚‹ã€‚

      :arg df: ä¿å­˜ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ 
      """
      try:
         temp_table = "TEMP_AMEDAS_DATA"

         # ä¸€æ™‚ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’å‰Šé™¤ã™ã‚‹
         self.session.sql(f"DROP TABLE IF EXISTS {temp_table}").collect()

         # DataFrameã®å†…å®¹ã‚’ä¸€æ™‚ãƒ†ãƒ¼ãƒ–ãƒ«ã«ä¿å­˜ã™ã‚‹
         self.session.write_pandas(df, temp_table, overwrite=True)

         # ä¸€æ™‚ãƒ†ãƒ¼ãƒ–ãƒ«ã‹ã‚‰æœ¬ãƒ†ãƒ¼ãƒ–ãƒ«ã«ãƒ‡ãƒ¼ã‚¿ã‚’æŒ¿å…¥ã™ã‚‹
         insert_query = """INSERT INTO CORE.WITH_AMEDAS SELECT * FROM TEMP_AMEDAS_DATA"""
         self.session.sql(insert_query).collect()

         st.success("ãƒ‡ãƒ¼ã‚¿ãŒå‡ºåŠ›ç”¨ãƒ†ãƒ¼ãƒ–ãƒ«ã«ä¿å­˜ã•ã‚Œã¾ã—ãŸã€‚", icon="âœ…")

      except SnowparkSQLException as e:
         st.error("ãƒ‡ãƒ¼ã‚¿ä¿å­˜ä¸­ã«SQLã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚ãƒ†ãƒ¼ãƒ–ãƒ«æ§‹é€ ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚", icon="â›”")
         st.error(f"ã‚¨ãƒ©ãƒ¼å†…å®¹: {str(e)}")
      except Exception as e:
         st.error("ãƒ‡ãƒ¼ã‚¿ä¿å­˜ä¸­ã«äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚ã‚·ã‚¹ãƒ†ãƒ ç®¡ç†è€…ã«é€£çµ¡ã—ã¦ãã ã•ã„ã€‚", icon="â›”")
         st.error(f"ã‚¨ãƒ©ãƒ¼å†…å®¹: {str(e)}")

   def highlight_columns(self, col_name: str, df: pd.DataFrame) -> pd.io.formats.style.Styler:
      """æŒ‡å®šã•ã‚ŒãŸã‚«ãƒ©ãƒ ã«èƒŒæ™¯è‰²ã‚’é©ç”¨ã™ã‚‹"""
      # NEAREST_OBSERVATORYåˆ—ä»¥é™ã®ã‚»ãƒ«ã«èƒŒæ™¯è‰²ã‚’é©ç”¨
      return df.style.applymap(lambda val: 'background-color: #ddffdd', subset=pd.IndexSlice[:, [col_name]])

   def run(self) -> None:
      """ã‚¢ãƒ—ãƒªã®ãƒ¡ã‚¤ãƒ³å‡¦ç†ã‚’å®Ÿè¡Œ"""
      try:
         self.ui_handler.setup_page()

         # ãƒ†ãƒ¼ãƒ–ãƒ«é¸æŠéƒ¨åˆ†ã®å‡¦ç†(1ï¸âƒ£)
         is_input_table_set = self.ui_handler.select_table(self)

         # ä½æ‰€ã‚«ãƒ©ãƒ é¸æŠéƒ¨åˆ†ã®å‡¦ç†(2ï¸âƒ£)
         is_2_ok, address_columns = self.ui_handler.select_address_column(is_input_table_set, self.input_df)

         # æ—¥æ¬¡ãƒ‡ãƒ¼ã‚¿ã®ã‚¿ã‚¤ãƒ—ã¨æœŸé–“ã®é¸æŠéƒ¨åˆ†ã®å‡¦ç†(3ï¸âƒ£)
         is_3_ok, start_date, end_date = self.ui_handler.select_date_range(is_2_ok)

         # å®Ÿè¡Œãƒœã‚¿ãƒ³æŠ¼ä¸‹å¾Œã®å‡¦ç†
         if is_3_ok:
            self.ui_handler.execute_final_process(self, address_columns, start_date, end_date)

      except Exception as e:
         st.error("ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®å®Ÿè¡Œä¸­ã«äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚å†è©¦è¡Œã—ã¦ãã ã•ã„ã€‚", icon="â›”")
         st.error(f"ã‚¨ãƒ©ãƒ¼å†…å®¹: {str(e)}")

# ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®å®Ÿè¡Œ
if __name__ == "__main__":

   # ! ğŸ”½ã®å¤‰æ•°ã‚’Trueã«è¨­å®šã™ã‚‹ã¨ã€ãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰ã§å®Ÿè¡Œã•ã‚Œã¾ã™
   test_mode = True
   # ! ğŸ”¼ã®å¤‰æ•°ã‚’Trueã«è¨­å®šã™ã‚‹ã¨ã€ãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰ã§å®Ÿè¡Œã•ã‚Œã¾ã™

   try:
      session = get_active_session()
      amedas_app = AMeDASDataMaker(session, "consumer_input_table")
      amedas_app.run()
   except Exception as e:
      st.error("ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®åˆæœŸåŒ–ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚ã‚·ã‚¹ãƒ†ãƒ ç®¡ç†è€…ã«é€£çµ¡ã—ã¦ãã ã•ã„ã€‚", icon="â›”")
      st.error(f"ã‚¨ãƒ©ãƒ¼å†…å®¹: {str(e)}")
